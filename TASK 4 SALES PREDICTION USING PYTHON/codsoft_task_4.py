# -*- coding: utf-8 -*-
"""CODSOFT_Task 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ORl_v0k0TueSOa_oqGFuGJtfY4DnKT-s

# CODSOFT Interbship TASK 4: SALES PREDICTION USING PYTHON

**Problem Statement**

Build a model which predicts sales based on the money spent on different platforms for marketing.

## Reading and Understanding the Data
"""

# Import the numpy and pandas package

import numpy as np
import pandas as pd

# Data Visualisation
import matplotlib.pyplot as plt
import seaborn as sns

advertising = pd.DataFrame(pd.read_csv("advertising.csv"))
advertising.head()

"""## Data Inspection"""

advertising.shape

advertising.info()

advertising.describe()

"""## Data Cleaning"""

# Checking Null values
advertising.isnull().sum()*100/advertising.shape[0]
# There are no NULL values in the dataset, hence it is clean.

# Outlier Analysis
fig, axs = plt.subplots(3, figsize = (5,5))
plt1 = sns.boxplot(advertising['TV'], ax = axs[0])
plt2 = sns.boxplot(advertising['Newspaper'], ax = axs[1])
plt3 = sns.boxplot(advertising['Radio'], ax = axs[2])
plt.tight_layout()

"""There are no considerable outliers present in the data.

## Exploratory Data Analysis

### Univariate Analysis

Sales (Target Variable)
"""

sns.boxplot(advertising['Sales'])
plt.show()

# Let's see how Sales are related with other variables using scatter plot.
sns.pairplot(advertising, x_vars=['TV', 'Newspaper', 'Radio'], y_vars='Sales', height=4, aspect=1, kind='scatter')
plt.show()

# Let's see the correlation between different variables.
sns.heatmap(advertising.corr(), cmap="YlGnBu", annot = True)
plt.show()

"""As is visible from the pairplot and the heatmap, the variable TV seems to be most correlated with Sales. So let's go ahead and perform simple linear regression using TV as our feature variable.

## Model Building

Performing Simple Linear Regression

Equation of linear regression
y = c + m1x1 + m2x2 +...+ mnxn


y
  is the response

c
  is the intercept

m1
  is the coefficient for the first feature

mn
  is the coefficient for the nth feature
In our case:

y =  c + m1 × TV


The  m
  values are called the model coefficients or model parameters.

**Generic Steps in model building using statsmodels :**

We first assign the feature variable, TV, in this case, to the variable X and the response variable, Sales, to the variable y.
"""

X = advertising['TV']
y = advertising['Sales']

"""**Train-Test Split**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)

# Let's now take a look at the train dataset

X_train.head()

y_train.head()

"""## Building a Linear Model"""

import statsmodels.api as sm

# Add a constant to get an intercept
X_train_sm = sm.add_constant(X_train)

# Fit the resgression line using 'OLS'
lr = sm.OLS(y_train, X_train_sm).fit()

# Print the parameters, i.e. the intercept and the slope of the regression line fitted
lr.params

# Performing a summary operation lists out all the different parameters of the regression line fitted
print(lr.summary())

"""### Looking at some key statistics from the summary

**The values we are concerned with are** -

1. The coefficients and significance (p-values)
2. R-squared
3. F statistic and its significance



* The coefficient for TV is 0.054, with a very low p value
The coefficient is statistically significant. So the association is not purely by chance.

*  R - squared is 0.816
Meaning that 81.6% of the variance in Sales is explained by TV.
This is a decent R-squared value.

*  F statistic has a very low p value (practically low)
Meaning that the model fit is statistically significant, and the explained variance isn't purely by chance.

The fit is significant.

Let's visualize how well the model fit the data.

From the parameters that we get, our linear regression equation becomes:

Sales = 6.948 + 0.054×TV
"""

plt.scatter(X_train, y_train)
plt.plot(X_train, 6.948 + 0.054*X_train, 'r')
plt.show()

"""## Model Evaluation"""

y_train_pred = lr.predict(X_train_sm)
res = (y_train - y_train_pred)

fig = plt.figure()
sns.distplot(res, bins = 15)
fig.suptitle('Error Terms', fontsize = 15)                  # Plot heading
plt.xlabel('y_train - y_train_pred', fontsize = 15)         # X-label
plt.show()

"""The residuals are following the normally distributed with a mean 0. All good!

Looking for patterns in the residuals
"""

plt.scatter(X_train,res)
plt.show()

"""We are confident that the model fit isn't by chance, and has decent predictive power. The normality of residual terms allows some inference on the coefficients.

Although, the variance of residuals increasing with X indicates that there is significant variation that this model is unable to explain.

As we can see, the regression line is a pretty good fit to the data

## Predictions on the Test Set

Now that we have fitted a regression line on our train dataset, it's time to make some predictions on the test data. For this, we first need to add a constant to the X_test data like we did for X_train and then we can simply go on and predict the y values corresponding to X_test using the predict attribute of the fitted regression line.
"""

# Add a constant to X_test
X_test_sm = sm.add_constant(X_test)

# Predict the y values corresponding to X_test_sm
y_pred = lr.predict(X_test_sm)

y_pred.head()

from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

"""Looking at the RMSE"""

#Returns the mean squared error; we'll take a square root
np.sqrt(mean_squared_error(y_test, y_pred))

"""Checking the R-squared on the test set"""

r_squared = r2_score(y_test, y_pred)
r_squared

"""Visualizing the fit on the test set"""

plt.scatter(X_test, y_test)
plt.plot(X_test, 6.948 + 0.054 * X_test, 'r')
plt.show()