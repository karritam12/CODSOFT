# -*- coding: utf-8 -*-
"""CODSOFT_Task 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14oPiRutXcRONKWM57sa5_oL2aRceNNDO

# CODSOFT Internship TASK 5: CREDIT CARD FRAUD DETECTION

## Brief Introduction

The dataset used for credit card fraud detection consists of anonymized features for transactions, including both legitimate and fraudulent ones. It's typically a binary classification problem where the goal is to predict whether a given transaction is fraudulent or not based on various features, such as transaction amount and time. It is highly imbalanced, with a much smaller proportion of fraudulent transactions. This makes it a challenging dataset for training predictive models. The notebook likely explores feature engineering, model selection, and evaluation techniques.

##  Importing Libraries and Dataset
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import numpy as np

df = pd.read_csv('creditcard.csv')
df.head()

"""##  Exploratory Data Analysis (EDA)"""

df.describe()

df.info()

df.shape

df.isnull().sum()

df.duplicated().sum()

df1 = df.drop_duplicates()

df1.shape

sns.countplot(x='Class', data=df1)

df1['Class'].value_counts()

plt.figure(figsize=(15,15))
t = 1
for i in df1.columns:
  plt.subplot(7,5,t)
  sns.histplot(df1[i], kde= True)
  plt.title(i+' Distribution')
  t+= 1
plt.tight_layout()
plt.show()

df_corr = df.corr()
plt.figure(figsize =(15,15))
sns.heatmap(df_corr, annot= False, cmap='coolwarm')

df['Time'].hist(bins = 50, color = 'purple', alpha = 0.7)

df1.isnull().sum()

"""## Data Preprocessing"""

for col in ['V24','V26','Amount']:
  q1 = df[col].quantile(0.25)
  q3 = df[col].quantile(0.75)
  iqr = q3 - q1
  l = q1 - 1.5 * iqr
  u = q3 + 1.5 * iqr
  df = df[(df[col] >= l) & (df[col] <= u)]

for col in ['V24','V26','Amount']:
  q1 = df1[col].quantile(0.25)
  q3 = df1[col].quantile(0.75)
  iqr = q3 - q1
  l = q1 - 1.5 * iqr
  u = q3 + 1.5 * iqr
  df1 = df1[(df1[col] >= l) & (df1[col] <= u)]

df1.shape

df1.shape

df1['Class'].value_counts()

X1 = df1.drop('Class', axis = 1)
y1 = df1['Class']

X = df.drop('Class', axis = 1)
y = df['Class']

df3 = df1.copy()

"""## üå≥ First Model

**We will apply anomaly detection using Isolation Forest (Unsupervised way!)**
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

print(f"Train size: {len(y_train)}, Dev size: {len(y_dev)}, Test size: {len(y_test)}")

model = IsolationForest(contamination=0.01, n_estimators=100, max_samples=0.7, random_state=42)
model.fit(X_train)

# Get anomaly scores and predictions
dev_scores = -model.decision_function(X_dev)
y_pred = model.predict(X_dev)  # -1 = anomaly, 1 = normal

from sklearn.metrics import precision_score, recall_score, f1_score
#Generate thresholds between min and max dev scores

thresholds = np.linspace(min(dev_scores), max(dev_scores), 100)
best_f1 = 0
best_threshold = 0

for t in thresholds:
    y_pred = (dev_scores >= t).astype(int)
    precision = precision_score(y_dev, y_pred)
    recall = recall_score(y_dev, y_pred)
    f1 = f1_score(y_dev, y_pred)

    if f1 > best_f1:
        best_f1 = f1
        best_threshold = t

print(f"Best Threshold: {best_threshold}")
print(f"Best F1 Score: {best_f1}")

test_scores = -model.decision_function(X_test)

# Use the best threshold to classify anomalies
final_y_pred = (test_scores >= best_threshold).astype(int)

from sklearn.metrics import confusion_matrix, classification_report

print("Confusion Matrix:")
print(confusion_matrix(y_test, final_y_pred))

print("Classification Report:")
print(classification_report(y_test, final_y_pred))

"""## ü§ñ Second Model

**We will use XGBoost this time (Supervised way!)**
"""

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

xgb = XGBClassifier(n_estimators=100,learning_rate= 0.3 ,max_depth= 3 ,random_state=42)
xgb.fit(X_train, y_train)

y_pred = xgb.predict(X_test)
print(classification_report(y_test, y_pred))

"""## ‚öñÔ∏è Comparing Unsupervised Vs. Supervised Models

Given the heavy class imbalance in the dataset, focusing on Class 1 (Anomalies) helps assess how well the model identifies the minority class. This approach is crucial for evaluating model performance, especially in fraud detection, where detecting the smaller class (fraudulent transactions) is vital for real-world effectiveness. This evaluation can help ensure that the model isn't biased towards the majority class.
"""

data = {
    "Model": ["Isolation Forest", "XGBoost"],
    "F1 Score": [0.59, 0.87]
}

df = pd.DataFrame(data)

fig = px.bar(
    df,
    x="Model",
    y="F1 Score",
    text="F1 Score",
    title="F1 Score Comparison on Test Dataset",
    labels={"F1 Score": "F1 Score", "Model": "Model"},
    color="Model",
    color_discrete_sequence=["deeppink", "turquoise"]
)

fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')
fig.update_layout(
    yaxis=dict(title="F1 Score", range=[0, 1]),
    xaxis=dict(title="Model"),
    title=dict(x=0.5),
    showlegend=False
)

"""## Conclusion

The supervised approach outperformed in terms of accuracy, but both methods have their unique advantages. The unsupervised model can identify hidden patterns not addressed in the dataset, while the supervised model is limited to predicting issues already present in the data. Both methods complement each other and could be used together for more comprehensive fraud detection systems.
"""